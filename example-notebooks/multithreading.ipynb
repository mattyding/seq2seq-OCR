{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multithreading\n",
    "\n",
    "This notebook contains an example of how to implement a multithreaded version of the seq2seq model over a directory of text files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import threading\n",
    "from threading import Thread\n",
    "\n",
    "# these two lines help with locating the file from this notebook\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from seq2seqocr import Seq2SeqOCR\n",
    "\n",
    "model = Seq2SeqOCR()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-13 14:09:52.250393: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the os.walk() function to create a queue of files to process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "paths_to_test = []\n",
    "\n",
    "for root, dirs, files in os.walk('sample-files/sample-directory'):  # these files are from 10/4/1820 London Times\n",
    "    for f in files:\n",
    "        if f.endswith(\".txt\"):\n",
    "            paths_to_test.append(root + \"/\" + f)\n",
    "\n",
    "paths_to_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sample-files/sample-directory/parlimentary1.txt',\n",
       " 'sample-files/sample-directory/parlimentary2.txt',\n",
       " 'sample-files/sample-directory/parlimentary3.txt',\n",
       " 'sample-files/sample-directory/bombay.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print out some of the first file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sample_file = paths_to_test[0]\n",
    "\n",
    "with open(sample_file, 'r') as f:\n",
    "        print(' '.join(f.read().split()[:45]) + ' [...]') # print first 45 words of one file"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PARLIAMENTARY INTELLIGENCE HOUSE OF LORDS, TUESDAY, OCT. 3. This day tbe house re-assembled, pursuant to adjournment. The peers began to take their places soon after nine o'clock, and at about seven minutes before ten the Lord-Chancellor entered, and seated himself on the woolsack. Prayers were [...]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define a helper function that houses a model.process_text() call within it and prints the result. We can spawn new threads running this function over all of the files in our queue.  \n",
    "\n",
    "NOTE: the below code will take a while to run (1-2 minutes on my local machine). Please be patient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def threaded_process(input_text : str) -> None:\n",
    "    processed_text = model.process_text(input_text)\n",
    "    # we can do anything with the text here -- save to an array, process it further, etc.\n",
    "    # I'm just going to print its contents with this gross-looking print statement\n",
    "    print(\"Size of file: \", len(input_text), ' words.\\n', ' '.join(processed_text.split()[:45]) + ' [...]\\n')\n",
    "\n",
    "\n",
    "for filepath in paths_to_test:\n",
    "    with open(filepath, 'r') as f:\n",
    "        file_text = f.read()\n",
    "        # spawn a new thread, processing each file\n",
    "        t = Thread(target=threaded_process, args=(file_text,))\n",
    "        t.start()\n",
    "        f.close()\n",
    "    \n",
    "    def wait_all_threads():\n",
    "        # citation: https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished\n",
    "        for t in threading.enumerate():\n",
    "            if t.daemon:\n",
    "                continue\n",
    "            try:\n",
    "                t.join()\n",
    "            except RuntimeError as err:\n",
    "                if 'cannot join current thread' in err.args[0]:\n",
    "                    # catchs main thread\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception(\"An error occurred while joining threads\")\n",
    "    \n",
    "    wait_all_threads()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-13 14:09:55.758734: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of file:  640  words.\n",
      " parliamentary intelligence house of lords tuesday oct this day tbe house reassembled pursuant to adjournment the peers began to take their places soon after nine oclock and at about seven minutes before ten the lord chancellor entered and seated himself on the wool sack prayers [...]\n",
      "\n",
      "Size of file:  1939  words.\n",
      " signor mariettithe earl of liverpool before their lord ships proceeded to thc business of thc day wished to say a few words on a transaction which had been madc the subject of conversation previously to the adjournment he alluded to what had passed respecting a [...]\n",
      "\n",
      "Size of file:  1646  words.\n",
      " expense of tlhe priscee dirgs against theiueenlhe earl of darnley prose to move thaan nccolnt of the money expendced in the proceedings against her mlajetay be laid before the noa se lhe had before urged tile prdoriety of thleir lord ships calling to a statdnient [...]\n",
      "\n",
      "Size of file:  2092  words.\n",
      " wuu bomlbay direct with leave to call at imatleiran loading andfd liii hill in october the ver fast sall i ship alexfkntflr biurden toi thomas urplencommander lsaponp and verysulerior aceonidationforpasi lycens flnd carresr ascipcreofor freight or p ssage apply to john five in cowcrsourtconhillora me [...]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some of the threads took longer to finish because their files have more text to be evaluated. However, this threaded program still processes the directory in a reasonably-fast time.  \n",
    "\n",
    "Below is an example of a threading program that takes every file in ORIGINAL_DIR, processes it, and then writes a new version of that file to PREDICTED_DIR. Some lines are commented out so as to not have this program create new files in the sample directory. To use this code, un-comment the lines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from threading import Lock\n",
    "\n",
    "printLock = Lock()\n",
    "\n",
    "# edit these to point at your data\n",
    "ORIGINAL_DIR = 'sample-files/'\n",
    "PREDICTED_DIR = 'predicted-files/'\n",
    "\n",
    "def seq2seq_process_direc(original_dir, predicted_dir, replace=False):\n",
    "    '''\n",
    "    Program that processes all files in a directory.\n",
    "\n",
    "    Un-comment out the lines below if you are using this on your own files.\n",
    "    '''\n",
    "    model = Seq2SeqOCR()\n",
    "\n",
    "    #if not os.path.isdir(predicted_dir):\n",
    "        #os.mkdir(predicted_dir)\n",
    "\n",
    "    paths_to_test = []\n",
    "\n",
    "    for root, dirs, files in os.walk(ORIGINAL_DIR):\n",
    "        old_file_dir = root.split('/')[-1]\n",
    "        processed_file_dir = os.path.join(PREDICTED_DIR, old_file_dir)\n",
    "        #if not os.path.isdir(processed_file_dir):\n",
    "            #os.mkdir(processed_file_dir)\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                paths_to_test.append((root, processed_file_dir, f))\n",
    "    \n",
    "    while (len(paths_to_test) > 0):\n",
    "        root, processed_file_dir, filename = paths_to_test.pop()\n",
    "        old_filepath = os.path.join(root, filename)\n",
    "        new_filepath = os.path.join(processed_file_dir, filename)\n",
    "\n",
    "        if (not replace) and (os.path.exists(new_filepath)):\n",
    "            s_print(\"File already exists: \" + new_filepath)\n",
    "            continue\n",
    "\n",
    "        t = Thread(target=threaded_newfile_process, args=(old_filepath, new_filepath, model))\n",
    "        t.start()\n",
    "    \n",
    "    wait_all_threads()\n",
    "\n",
    "\n",
    "def threaded_newfile_process(old_filepath, new_filepath, model):\n",
    "    '''\n",
    "    spawns a new thread that process a single file and write its output to a new file\n",
    "    '''\n",
    "    with open(old_filepath, 'r') as f:\n",
    "        orig_text = f.read()\n",
    "        f.close()\n",
    "\n",
    "    #processed_text = model.process_text(orig_text)\n",
    "\n",
    "    #with open(new_filepath, 'w') as f:\n",
    "        #f.write(processed_text)\n",
    "        #f.close()\n",
    "    \n",
    "    s_print(\"Created new file: \" +  new_filepath)\n",
    "\n",
    "def s_print(msg):\n",
    "    '''\n",
    "    thread-safe print\n",
    "    '''\n",
    "    printLock.acquire()\n",
    "    try:\n",
    "        print(msg)\n",
    "    finally:\n",
    "        printLock.release()\n",
    "\n",
    "\n",
    "seq2seq_process_direc(ORIGINAL_DIR, PREDICTED_DIR)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Created new file: predicted-files/sample-directory/parlimentary2.txt\n",
      "Created new file: predicted-files/sample-directory/parlimentary3.txt\n",
      "Created new file: predicted-files/sample-directory/parlimentary1.txt\n",
      "Created new file: predicted-files/sample-directory/bombay.txt\n",
      "Created new file: predicted-files/floods.txt\n",
      "Created new file: predicted-files/floods_correct.txt\n",
      "Created new file: predicted-files/floods_paired.txt\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "0df766557e9e52c50ed33f63d0ce7ef29166bb38e4b1cc05d489187aa19089d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}