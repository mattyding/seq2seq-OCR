{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multithreading\n",
    "\n",
    "This notebook contains an example of how to implement a multithreaded version of the seq2seq model over a directory of text files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import threading\n",
    "from threading import Thread\n",
    "\n",
    "# these two lines help with locating the file from this notebook\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from seq2seqocr import Seq2SeqOCR\n",
    "\n",
    "model = Seq2SeqOCR()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-06 02:00:50.767377: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the os.walk() function to create a queue of files to process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "paths_to_test = [] # list of tuples in the format (root, file)\n",
    "\n",
    "for root, dirs, files in os.walk('sample-files/sample-directory'):  # these files are from 10/4/1820 London Times\n",
    "    for f in files:\n",
    "        if f.endswith(\".txt\"):\n",
    "            paths_to_test.append(root + \"/\" + f)\n",
    "\n",
    "paths_to_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sample-files/sample-directory/parlimentary1.txt',\n",
       " 'sample-files/sample-directory/parlimentary2.txt',\n",
       " 'sample-files/sample-directory/parlimentary3.txt',\n",
       " 'sample-files/sample-directory/bombay.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print out some of the first file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sample_file = paths_to_test[0]\n",
    "\n",
    "with open(sample_file, 'r') as f:\n",
    "        print(' '.join(f.read().split()[:45]) + ' [...]') # print first 45 words of one file"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PARLIAMENTARY INTELLIGENCE HOUSE OF LORDS, TUESDAY, OCT. 3. This day tbe house re-assembled, pursuant to adjournment. The peers began to take their places soon after nine o'clock, and at about seven minutes before ten the Lord-Chancellor entered, and seated himself on the woolsack. Prayers were [...]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define a helper function that houses a model.process_text() call within it and prints the result. We can spawn new threads running this function over all of the files in our queue.  \n",
    "\n",
    "NOTE: the below code will take a while to run (1-2 minutes on my local machine). Please be patient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def threaded_process(input_text : str) -> None:\n",
    "    processed_text = model.process_text(input_text)\n",
    "    # we can do anything with the text here -- save to an array, process it further, etc.\n",
    "    # I'm just going to print its contents with this gross-looking print statement\n",
    "    print(\"Size of file: \", len(input_text), ' words.\\n', ' '.join(processed_text.split()[:45]) + ' [...]\\n')\n",
    "\n",
    "\n",
    "for filepath in paths_to_test:\n",
    "    with open(filepath, 'r') as f:\n",
    "        file_text = f.read()\n",
    "        # spawn a new thread, processing each file\n",
    "        t = Thread(target=threaded_process, args=(file_text,))\n",
    "        t.start()\n",
    "        f.close()\n",
    "    \n",
    "# citation: https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished\n",
    "for t in threading.enumerate():\n",
    "    if t.daemon:\n",
    "        continue\n",
    "    try:\n",
    "        t.join()\n",
    "    except RuntimeError as err:\n",
    "        if 'cannot join current thread' in err.args[0]:\n",
    "            # catchs main thread\n",
    "            continue\n",
    "        else:\n",
    "            raise Exception(\"An error occurred while joining threads\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-06 02:00:54.726426: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of file:  640  words.\n",
      " parliamentary intelligence house of lords tuesday oct this day tbe house reassembled pursuant to adjournment the peers began to take their places soon after nine oclock and at about seven minutes before ten the lord chancellor entered and seated himself on the wool sack prayers [...]\n",
      "\n",
      "Size of file:  1939  words.\n",
      " signor mariettithe earl of liverpool before their lord ships proceeded to thc business of thc day wished to say a few words on a transaction which had been made the subject of conversation previously to the adjournment he alluded to what had passed respecting a [...]\n",
      "\n",
      "Size of file:  1646  words.\n",
      " expense of title priscee dirgs against theiueenlhe earl of darnley prose to move thaan nccolnt of the money expendced in the proceedings against her mlajetay be laid before the noa se lhe had before urged tile prdoriety of thleir lord ships calling to a statdnient [...]\n",
      "\n",
      "Size of file:  2092  words.\n",
      " wuu bomlbay direct with leave to call at imatleiran loading andfd liii hsil in october the ver fast sall i ship alexfkntflr biurden toi thomas urplencommander lsaponp and verysulerior aceonidationforpasi lycens flnd carresr ascipcreofor freight or p ssage apply to jobi lyoc in cowcrsourtconhillora me [...]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some of the threads took longer to finish because their files have more text to be evaluated. However, this threaded program still processes the directory in a reasonably-fast time.\n",
    "\n",
    "That is all. I hope this notebook is a useful demonstration of applying the model to processing large directories of text."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "0df766557e9e52c50ed33f63d0ce7ef29166bb38e4b1cc05d489187aa19089d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}